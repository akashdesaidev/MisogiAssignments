# Prompt Engineering Pipeline: Multi-Path Reasoning + Automated Prompt Optimization

## ğŸ¯ Objective
This project implements a sophisticated prompt engineering pipeline that combines:
- **Tree-of-Thought (ToT)** reasoning for multi-path exploration
- **Self-Consistency** for robust answer aggregation
- **Automated Prompt Optimization** using feedback loops

## ğŸ—ï¸ Architecture

```
Pipeline Flow:
Query â†’ ToT Reasoning â†’ Self-Consistency â†’ Automated Optimization â†’ Refined Results
```

### Components:
1. **Multi-Path Reasoning**: Generate N reasoning paths using Tree-of-Thought
2. **Self-Consistency**: Aggregate answers through majority voting/consensus
3. **Automated Optimization**: OPRO/TextGrad-style prompt refinement
4. **Evaluation**: Comprehensive metrics and reflection analysis

## ğŸ“ Project Structure

```
q2/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ tasks/                       # Problem definitions
â”‚   â”œâ”€â”€ math_problems.json
â”‚   â”œâ”€â”€ logic_puzzles.json
â”‚   â”œâ”€â”€ code_debugging.json
â”‚   â””â”€â”€ task_definitions.md
â”œâ”€â”€ prompts/                     # Prompt templates
â”‚   â”œâ”€â”€ base_prompts.json
â”‚   â”œâ”€â”€ tot_prompts.json
â”‚   â”œâ”€â”€ optimization_prompts.json
â”‚   â””â”€â”€ prompt_history.json
â”œâ”€â”€ src/                         # Core pipeline code
â”‚   â”œâ”€â”€ main.py                  # Main pipeline orchestrator
â”‚   â”œâ”€â”€ tot_reasoning.py         # Tree-of-Thought implementation
â”‚   â”œâ”€â”€ self_consistency.py     # Self-consistency aggregation
â”‚   â”œâ”€â”€ prompt_optimizer.py     # Automated prompt optimization
â”‚   â”œâ”€â”€ evaluator.py             # Evaluation metrics
â”‚   â””â”€â”€ utils.py                 # Utility functions
â”œâ”€â”€ logs/                        # Execution logs
â”‚   â”œâ”€â”€ reasoning_paths/
â”‚   â”œâ”€â”€ optimization_history/
â”‚   â””â”€â”€ performance_metrics/
â””â”€â”€ evaluation/                  # Results and analysis
    â”œâ”€â”€ metrics_report.md
    â”œâ”€â”€ reflection_analysis.md
    â””â”€â”€ performance_data.json
```

## ğŸš€ Setup & Installation

### Prerequisites
- Python 3.8+
- OpenAI API key (or local LLM setup)
- Required Python packages

### Installation
```bash
cd Assignment3/q2
pip install -r requirements.txt
```

### Configuration
1. Set up your LLM API credentials in environment variables
2. Configure pipeline parameters in `src/config.py`

## ğŸ® Usage

### Basic Pipeline Execution
```bash
python src/main.py --task math_problems --optimize --verbose
```

### Advanced Options
```bash
# Run with specific parameters
python src/main.py \
    --task logic_puzzles \
    --tot-paths 5 \
    --consistency-threshold 0.7 \
    --optimization-rounds 3 \
    --save-logs
```

### Available Tasks
- `math_problems`: Multi-step mathematical reasoning
- `logic_puzzles`: Logical deduction problems
- `code_debugging`: Code analysis and debugging
- `planning_tasks`: Sequential planning problems
- `causal_reasoning`: Cause-and-effect analysis

## ğŸ“Š Key Features

### Tree-of-Thought Reasoning
- Generates multiple reasoning branches
- Evaluates and prunes paths dynamically
- Maintains reasoning coherence

### Self-Consistency
- Aggregates multiple reasoning paths
- Uses majority voting and consensus mechanisms
- Handles confidence scoring

### Automated Optimization
- Detects reasoning failures and inconsistencies
- Refines prompts using feedback loops
- Tracks optimization history and performance

## ğŸ“ˆ Evaluation Metrics

- **Task Accuracy**: Correctness of final answers
- **Reasoning Coherence**: Quality of reasoning paths
- **Hallucination Rate**: Frequency of factual errors
- **Optimization Improvement**: Performance gains from prompt refinement
- **Consistency Score**: Agreement across reasoning paths

## ğŸ” Results & Insights

See `evaluation/` directory for:
- Detailed performance analysis
- Optimization effectiveness studies
- Trade-off analysis (cost vs. quality)
- Reflection on methodology

## ğŸ› ï¸ Development

### Running Tests
```bash
python -m pytest tests/
```

### Adding New Tasks
1. Define task in `tasks/` directory
2. Add task-specific prompts in `prompts/`
3. Update task registry in `src/main.py`

## ğŸ“ License
This project is part of an academic assignment and is for educational purposes. 